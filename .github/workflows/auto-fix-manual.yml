name: Auto Fix Manual Trigger

on:
    workflow_dispatch:
        inputs:
            article_id:
                description: "Optional: process only this article ID"
                required: false
                default: ""

concurrency:
    group: auto-fix-manual
    cancel-in-progress: false

jobs:
    auto-fix-one:
        runs-on: ubuntu-latest
        timeout-minutes: 45
        permissions:
            contents: write
        env:
            SHOPIFY_SHOP: ${{ secrets.SHOPIFY_SHOP }}
            SHOPIFY_STORE_DOMAIN: ${{ secrets.SHOPIFY_SHOP }}
            SHOPIFY_ACCESS_TOKEN: ${{ secrets.SHOPIFY_ACCESS_TOKEN }}
            SHOPIFY_BLOG_ID: ${{ secrets.SHOPIFY_BLOG_ID }}
            SHOPIFY_API_VERSION: ${{ secrets.SHOPIFY_API_VERSION }}
            POLLINATIONS_API_KEY: ${{ secrets.POLLINATIONS_API_KEY }}
            GET_POLLINATIONS_URL: ${{ secrets.GET_POLLINATIONS_URL }}
            POLLINATIONS_NEGATIVE: "text, watermark, logo, signature"
            USE_LEXICA: "0"
            LEXICA_ONLY_SD15: "1"
            LEXICA_RESULT_LIMIT: "30"
            LEXICA_FALLBACK_ONLY: "1"
            VISION_REVIEW: "1"
            VISION_API_KEY: ${{ secrets.VISION_API_KEY }}
            GOOGLE_AI_STUDIO_API_KEY: ${{ secrets.GOOGLE_AI_STUDIO_API_KEY }}
            GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
            FALLBACK_GEMINI_API_KEY: ${{ secrets.FALLBACK_GEMINI_API_KEY }}
            FALLBACK_GOOGLE_AI_STUDIO_API_KEY: ${{ secrets.FALLBACK_GEMINI_API_KEY }}
            SECOND_FALLBACK_GEMINI_API_KEY: ${{ secrets.SECOND_FALLBACK_GEMINI_API_KEY || secrets.GEMINI_API_KEY_3 || secrets.THIRD_GEMINI_API_KEY || '' }}
            SECOND_FALLBACK_GOOGLE_AI_STUDIO_API_KEY: ${{ secrets.SECOND_FALLBACK_GEMINI_API_KEY || secrets.GEMINI_API_KEY_3 || secrets.THIRD_GEMINI_API_KEY || '' }}
            GEMINI_MODEL: ${{ vars.GEMINI_MODEL || 'gemini-2.0-flash' }}
            GEMINI_MODEL_FALLBACK: ${{ vars.GEMINI_MODEL_FALLBACK || 'gemini-2.5-flash-lite' }}
            GEMINI_MODEL_FALLBACK_2: ${{ vars.GEMINI_MODEL_FALLBACK_2 || 'gemini-2.5-flash' }}
            GEMINI_MODEL_FALLBACK_3: ${{ vars.GEMINI_MODEL_FALLBACK_3 || 'gemini-2.0-flash-lite' }}
            GEMINI_DELAY_SECONDS: "3"
            OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
            GH_MODELS_API_KEY: ${{ secrets.GH_MODELS_API_KEY }}
            GH_MODELS_API_BASE: ${{ vars.GH_MODELS_API_BASE || 'https://models.github.ai/inference' }}
            GH_MODELS_MODEL: ${{ vars.GH_MODELS_MODEL || 'openai/gpt-4.1' }}
            VISION_PROVIDER: ${{ vars.VISION_PROVIDER || '' }}
            VISION_API_BASE: ${{ vars.VISION_API_BASE || 'https://models.github.ai/inference' }}
            VISION_MODEL_ID: ${{ vars.VISION_MODEL_ID || 'openai/gpt-4o-mini' }}
            VISION_TIMEOUT: "30"
            VISION_MAX_ATTEMPTS: "2"
            VISION_RETRY_SLEEP: "5"
            VISION_BACKOFF_FACTOR: "1.8"
            GCP_PROJECT: ${{ vars.GCP_PROJECT || '' }}
            GCP_LOCATION: ${{ vars.GCP_LOCATION || 'us-central1' }}
            GEMINI_IMAGE_MODEL: ${{ vars.GEMINI_IMAGE_MODEL || 'imagen-4.0-fast-generate-001' }}
            GOOGLE_APPLICATION_CREDENTIALS: ${{ github.workspace }}/gcp-sa.json
            GCP_SA_JSON: ${{ secrets.GCP_SA_JSON }}
            USE_VERTEX_IMAGEN: ${{ vars.USE_VERTEX_IMAGEN || '' }}
            REQUIRE_PINTEREST_IMAGE: "0"
            FIX_MAX_ITEMS: "1"
            FIX_DELAY_SECONDS: "60"
            PASS_RATE_ENFORCE: "1"
            MIN_PASS_RATE: "0.95"
            PASS_RATE_WINDOW: "20"
            PASS_RATE_MIN_SAMPLES: "10"
        steps:
            - name: Checkout
              uses: actions/checkout@v4

            - name: Setup Python
              uses: actions/setup-python@v5
              with:
                  python-version: "3.11"

            - name: Install dependencies
              run: |
                  python -m pip install --upgrade pip
                  python -m pip install requests beautifulsoup4 python-dotenv PyYAML google-auth pillow

            - name: Write GCP service account
              run: |
                  python - <<'PY'
                  import os
                  from pathlib import Path
                  data = os.environ.get("GCP_SA_JSON", "").strip()
                  if not data:
                      raise SystemExit(0)
                  path = os.environ.get("GOOGLE_APPLICATION_CREDENTIALS", "")
                  if not path:
                      raise SystemExit("GOOGLE_APPLICATION_CREDENTIALS is not set")
                  Path(path).write_text(data, encoding="utf-8")
                  print(f"Wrote service account to {path}")
                  PY

            - name: Verify vision review enabled
              run: |
                                    if [ "${VISION_REVIEW:-0}" = "1" ] \
                                        && [ -z "${VISION_API_KEY:-}" ] \
                                        && [ -z "${GOOGLE_AI_STUDIO_API_KEY:-}" ] \
                                        && [ -z "${GEMINI_API_KEY:-}" ] \
                                        && [ -z "${FALLBACK_GEMINI_API_KEY:-}" ] \
                                        && [ -z "${SECOND_FALLBACK_GEMINI_API_KEY:-}" ]; then
                                            echo "Missing VISION_API_KEY or any GEMINI API key; vision review is required."
                      exit 1
                  fi

            - name: Check if queue needs refresh
              id: queue_check
              working-directory: Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2
              run: |
                  python - <<'PY'
                  import json
                  import os
                  from pathlib import Path

                  output_path = os.environ.get('GITHUB_OUTPUT')

                  queue_path = Path('anti_drift_queue.json')
                  if not queue_path.exists():
                      if output_path:
                          with open(output_path, 'a', encoding='utf-8') as f:
                              f.write('needs_refresh=true\n')
                      raise SystemExit(0)

                  try:
                      data = json.loads(queue_path.read_text(encoding='utf-8'))
                  except json.JSONDecodeError:
                      if output_path:
                          with open(output_path, 'a', encoding='utf-8') as f:
                              f.write('needs_refresh=true\n')
                      raise SystemExit(0)

                  items = data.get('items', [])
                  # Auto-escalate stuck articles before counting
                  MAX_ESCALATE_ATTEMPTS = 5
                  for item in items:
                      if item.get('status') in ('failed', 'retrying', 'pending') and int(item.get('attempts', 0)) >= MAX_ESCALATE_ATTEMPTS:
                          item['status'] = 'manual_review'
                          item['last_error'] = f"AUTO_ESCALATED_AFTER_{item.get('attempts')}_ATTEMPTS: {item.get('last_error', '')}"
                  data['items'] = items
                  queue_path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding='utf-8')

                  pending_like = [
                      i for i in items
                      if i.get('status') in {'pending', 'failed', 'retrying'}
                  ]
                  needs_refresh = 'false' if pending_like else 'true'
                  if output_path:
                      with open(output_path, 'a', encoding='utf-8') as f:
                          f.write(f'needs_refresh={needs_refresh}\n')
                  PY

            - name: Scan issues (published)
              if: steps.queue_check.outputs.needs_refresh == 'true'
              working-directory: Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2
              run: |
                  SCAN_LIMIT=50 python scan_issues_now.py

            - name: Ensure articles_to_fix.json exists
              if: steps.queue_check.outputs.needs_refresh == 'true'
              id: fix_list
              working-directory: Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2
              run: |
                  python - <<'PY'
                  import json
                  import os
                  from pathlib import Path

                  path = Path('articles_to_fix.json')
                  if not path.exists():
                      path.write_text('[]', encoding='utf-8')

                  try:
                      items = json.loads(path.read_text(encoding='utf-8'))
                  except json.JSONDecodeError:
                      items = []
                      path.write_text('[]', encoding='utf-8')

                  has_items = 'true' if items else 'false'
                  output_path = os.environ.get('GITHUB_OUTPUT')
                  if output_path:
                      with open(output_path, 'a', encoding='utf-8') as f:
                          f.write(f'has_items={has_items}\n')
                  print(f'articles_to_fix count: {len(items)}')
                  PY

            - name: Rotate queue to avoid repeat
              if: steps.queue_check.outputs.needs_refresh == 'true' && steps.fix_list.outputs.has_items == 'true'
              working-directory: Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2
              run: |
                  if [ ! -s "articles_to_fix.json" ]; then
                      echo "No items to fix; skipping rotate."
                      exit 0
                  fi
                  python - <<'PY'
                  import json
                  import os
                  from pathlib import Path

                  path = Path('articles_to_fix.json')
                  if not path.exists():
                      print('articles_to_fix.json not found; skip rotate')
                      raise SystemExit(0)

                  items = json.loads(path.read_text(encoding='utf-8'))
                  if not items:
                      print('No items to fix')
                      raise SystemExit(0)

                  run_number = int(os.environ.get('GITHUB_RUN_NUMBER', '0'))
                  offset = run_number % len(items)
                  rotated = items[offset:] + items[:offset]
                  path.write_text(json.dumps(rotated, ensure_ascii=False, indent=2), encoding='utf-8')
                  print(f'Rotated queue by offset {offset} (run {run_number})')
                  PY

            - name: Init anti-drift queue
              if: steps.queue_check.outputs.needs_refresh == 'true' && steps.fix_list.outputs.has_items == 'true'
              working-directory: Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2
              run: |
                  if [ ! -s "articles_to_fix.json" ]; then
                      echo "No items to fix; skipping queue init."
                      exit 0
                  fi
                  python ai_orchestrator.py queue-init

            - name: Fix up to 50 articles sequentially (no batch)
              working-directory: Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2
              run: |
                  set -euo pipefail
                  if [ ! -f "anti_drift_queue.json" ]; then
                      echo "Queue missing; re-initializing."
                      python ai_orchestrator.py queue-init || true
                  fi
                  if [ ! -f "anti_drift_queue.json" ]; then
                      echo "No queue found; skipping fix loop."
                      exit 0
                  fi
                  manual_id="${{ github.event.inputs.article_id }}"
                  max=${FIX_MAX_ITEMS:-50}
                  delay=${FIX_DELAY_SECONDS:-600}
                  processed_file=".processed_ids"
                  : > "$processed_file"
                  for i in $(seq 1 "$max"); do
                      if [ -n "$manual_id" ]; then
                          article_id="$manual_id"
                      else
                          article_id=$(python - <<'PY'
                  import json
                  from pathlib import Path

                  queue_path = Path('anti_drift_queue.json')
                  if not queue_path.exists():
                      raise SystemExit('')

                  data = json.loads(queue_path.read_text(encoding='utf-8'))
                  items = data.get('items', [])
                  MAX_PICK_ATTEMPTS = 5
                  priority = ['pending', 'retrying', 'failed']
                  chosen = None
                  for status in priority:
                      for item in items:
                          if item.get('status') == status:
                              attempts = int(item.get('attempts', 0))
                              if attempts >= MAX_PICK_ATTEMPTS:
                                  continue  # Skip stuck articles
                              chosen = item
                              break
                      if chosen:
                          break

                  print(chosen.get('id') if chosen else '')
                  PY
                          )
                      fi

                      if [ -z "$article_id" ]; then
                          echo "No eligible items in queue."
                          break
                      fi

                      if grep -Fxq "$article_id" "$processed_file"; then
                          echo "Already processed $article_id in this run; skipping."
                          python - <<PY
                  import json
                  from pathlib import Path
                  article_id = "$article_id"
                  queue_path = Path('anti_drift_queue.json')
                  data = json.loads(queue_path.read_text(encoding='utf-8'))
                  for item in data.get('items', []):
                      if str(item.get('id')) == str(article_id):
                          item['status'] = 'skipped'
                          item['error'] = 'DUP_IN_RUN'
                  queue_path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding='utf-8')
                  PY
                          continue
                      fi

                      echo "Processing $article_id ($i/$max)"
                      echo "$article_id" >> "$processed_file"
                      set +e
                      python ai_orchestrator.py fix-ids "$article_id"
                      fix_ok=$?
                      if [ "$fix_ok" != "0" ]; then echo "[WARN] fix-ids $article_id exited $fix_ok"; fi

                      # NOTE: Removed build_meta_fix_queue.py and run_meta_fix_queue.py
                      # as ai_orchestrator.py now handles all meta-prompt fixes
                      python "fix_images_properly.py" --article-id "$article_id" --images-only
                      fix_images_ok=$?
                      if [ "$fix_images_ok" != "0" ]; then
                          echo "[WARN] fix_images_properly failed for $article_id"
                          python ai_orchestrator.py queue-review "$article_id" fail "IMAGE_FIX_FAILED" || true
                          if [ -n "$manual_id" ]; then
                              break
                          fi
                          if [ "$i" -lt "$max" ]; then
                              echo "Sleeping ${delay}s before next article..."
                              sleep "$delay"
                          fi
                          continue
                      fi
                      set -e

                      set +e
                      python "../scripts/pre_publish_review.py" "$article_id" | tee "review-output-$article_id.txt"
                      review_status=${PIPESTATUS[0]}
                      set -e

                      if [ "$review_status" != "0" ]; then
                          python ai_orchestrator.py force-rebuild-ids "$article_id"
                          python "fix_images_properly.py" --article-id "$article_id" --images-only
                          rebuild_images_ok=$?
                          if [ "$rebuild_images_ok" != "0" ]; then
                              echo "[WARN] fix_images_properly failed after rebuild for $article_id"
                              python ai_orchestrator.py queue-review "$article_id" fail "IMAGE_FIX_FAILED" || true
                              if [ -n "$manual_id" ]; then
                                  break
                              fi
                              if [ "$i" -lt "$max" ]; then
                                  echo "Sleeping ${delay}s before next article..."
                                  sleep "$delay"
                              fi
                              continue
                          fi
                          set +e
                          python "../scripts/pre_publish_review.py" "$article_id" | tee "review-output-${article_id}-after-rebuild.txt"
                          review_status_after=${PIPESTATUS[0]}
                          set -e
                          if [ "$review_status_after" != "0" ]; then
                              python ai_orchestrator.py queue-review "$article_id" fail "PRE_PUBLISH_REVIEW_FAIL" || true
                              echo "Pre-publish review failed after rebuild for $article_id. Skipping."
                          fi
                      fi
                      if [ "$review_status" == "0" ] || [ "${review_status_after:-1}" == "0" ]; then
                          echo "Review passed for $article_id. Cleanup then publish..."
                          python cleanup_before_publish.py "$article_id" || true
                          python publish_now_graphql.py "$article_id" || true
                          python ai_orchestrator.py queue-review "$article_id" done "PUBLISHED" || true
                          echo "Published and marked done: $article_id"
                      fi
                      if [ -n "$manual_id" ]; then
                          break
                      fi
                      if [ "$i" -lt "$max" ]; then
                          echo "Sleeping ${delay}s before next article..."
                          sleep "$delay"
                      fi
                  done

            - name: Commit queue changes
              if: always()
              env:
                  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
              run: |
                  git config user.name "github-actions[bot]"
                  git config user.email "github-actions[bot]@users.noreply.github.com"
                  QUEUE_DIR="Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2"
                  if [ -f "$QUEUE_DIR/anti_drift_queue.json" ]; then
                      # Use --force to add files that are in .gitignore
                      git add --force "$QUEUE_DIR/anti_drift_queue.json" || echo "add queue failed"
                      git add --force "$QUEUE_DIR/anti_drift_run_log.csv" 2>/dev/null || true
                      git add --force "$QUEUE_DIR/anti_drift_done_blacklist.json" 2>/dev/null || true
                      git add --force "$QUEUE_DIR/orchestrator_progress.json" 2>/dev/null || true
                      echo "Files staged:"
                      git diff --cached --name-only || true
                      if ! git diff --cached --quiet; then
                          git commit -m "chore: Update queue state after manual run [skip ci]"
                          git remote set-url origin "https://x-access-token:${GITHUB_TOKEN}@github.com/${{ github.repository }}.git"
                          git push origin HEAD:${{ github.ref_name }} && echo "✅ Queue changes pushed successfully" || echo "❌ Push failed"
                      else
                          echo "No queue changes to commit (git diff --cached is empty)"
                      fi
                  else
                      echo "Queue file not found at $QUEUE_DIR/anti_drift_queue.json"
                  fi

            - name: Upload fix artifacts
              uses: actions/upload-artifact@v4
              with:
                  name: auto-fix-manual
                  path: |
                      Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2/anti_drift_queue.json
                      Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2/anti_drift_run_log.csv
                      Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2/orchestrator_progress.json
                      Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2/anti_drift_done_blacklist.json
                      Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2/review-output*.txt
                  if-no-files-found: warn
