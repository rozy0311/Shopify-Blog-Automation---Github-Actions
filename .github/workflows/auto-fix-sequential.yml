name: Auto Fix Sequential (Continuous Loop)
# Runs every 20 minutes (typical run time is ~10–16 minutes; avoid backlog)
# Batch FAST-SKIP: marks ALL already-published + passing articles as done in one pass
# Then processes up to 2 truly broken articles per run (within 45-min budget)
# cancel-in-progress: false = queue next scheduled run instead of cancelling

on:
    schedule:
        # Every 20 minutes — 1 article/run
        - cron: "*/20 * * * *"
    workflow_dispatch:
        inputs:
            fix_max_items:
                description: "Number of articles per run (default 1)"
                required: false
                default: "1"

concurrency:
    group: auto-fix-sequential-${{ github.ref }}
    cancel-in-progress: false

jobs:
    auto-fix-one:
        runs-on: ubuntu-latest
        timeout-minutes: 45
        permissions:
            contents: write
        env:
            SHOPIFY_SHOP: ${{ secrets.SHOPIFY_SHOP }}
            SHOPIFY_STORE_DOMAIN: ${{ secrets.SHOPIFY_SHOP }}
            SHOPIFY_ACCESS_TOKEN: ${{ secrets.SHOPIFY_ACCESS_TOKEN }}
            SHOPIFY_BLOG_ID: ${{ secrets.SHOPIFY_BLOG_ID }}
            SHOPIFY_API_VERSION: ${{ secrets.SHOPIFY_API_VERSION }}
            POLLINATIONS_API_KEY: ${{ secrets.POLLINATIONS_API_KEY }}
            GET_POLLINATIONS_URL: ${{ secrets.GET_POLLINATIONS_URL }}
            POLLINATIONS_NEGATIVE: "people, person, human, hands, fingers, face, portrait, selfie, deformed hands, extra fingers, mutated hands, poorly drawn hands, bad anatomy, extra limbs, fused fingers, too many fingers, missing fingers, deformed, disfigured, blurry, bad proportions, ugly, duplicate, morbid, mutilated, text, watermark, logo, signature, words, letters"
            USE_LEXICA: "0"
            LEXICA_ONLY_SD15: "1"
            LEXICA_RESULT_LIMIT: "30"
            LEXICA_FALLBACK_ONLY: "1"
            VISION_REVIEW: "0"
            VISION_API_KEY: ${{ secrets.VISION_API_KEY }}
            GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
            GOOGLE_AI_STUDIO_API_KEY: ${{ secrets.GOOGLE_AI_STUDIO_API_KEY }}
            FALLBACK_GEMINI_API_KEY: ${{ secrets.FALLBACK_GEMINI_API_KEY }}
            FALLBACK_GOOGLE_AI_STUDIO_API_KEY: ${{ secrets.FALLBACK_GEMINI_API_KEY }}
            SECOND_FALLBACK_GEMINI_API_KEY: ${{ secrets.SECOND_FALLBACK_GEMINI_API_KEY }}
            SECOND_FALLBACK_GOOGLE_AI_STUDIO_API_KEY: ${{ secrets.SECOND_FALLBACK_GOOGLE_AI_STUDIO_API_KEY }}
            GEMINI_MODEL: gemini-2.0-flash
            GEMINI_MODEL_FALLBACK: gemini-2.5-flash-lite
            GEMINI_MODEL_FALLBACK_2: gemini-2.5-flash
            GEMINI_MODEL_FALLBACK_3: gemini-2.0-flash-lite
            GEMINI_DELAY_SECONDS: "3"
            OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
            GH_MODELS_API_KEY: ${{ secrets.GH_MODELS_API_KEY }}
            GH_MODELS_MODEL: ${{ vars.GH_MODELS_MODEL || 'openai/gpt-4.1' }}
            VISION_PROVIDER: ${{ vars.VISION_PROVIDER || '' }}
            VISION_API_BASE: ${{ vars.VISION_API_BASE || 'https://models.github.ai/inference' }}
            VISION_MODEL_ID: ${{ vars.VISION_MODEL_ID || 'openai/gpt-4o-mini' }}
            VISION_TIMEOUT: ${{ vars.VISION_TIMEOUT || '30' }}
            VISION_MAX_ATTEMPTS: ${{ vars.VISION_MAX_ATTEMPTS || '6' }}
            VISION_RETRY_SLEEP: ${{ vars.VISION_RETRY_SLEEP || '6' }}
            VISION_BACKOFF_FACTOR: ${{ vars.VISION_BACKOFF_FACTOR || '1.8' }}
            GCP_PROJECT: ${{ vars.GCP_PROJECT || '' }}
            GCP_LOCATION: ${{ vars.GCP_LOCATION || 'us-central1' }}
            GEMINI_IMAGE_MODEL: ${{ vars.GEMINI_IMAGE_MODEL || 'imagen-4.0-fast-generate-001' }}
            GOOGLE_APPLICATION_CREDENTIALS: ${{ github.workspace }}/gcp-sa.json
            GCP_SA_JSON: ${{ secrets.GCP_SA_JSON }}
            USE_VERTEX_IMAGEN: ${{ vars.USE_VERTEX_IMAGEN || '' }}
            GEMINI_IMAGE_GEN_MODELS: "gemini-2.0-flash-exp-image-generation,gemini-2.5-flash-image,gemini-3-pro-image-preview"
            REQUIRE_PINTEREST_IMAGE: "0"
            FIX_MAX_ITEMS: ${{ github.event.inputs.fix_max_items || '1' }}
            FIX_DELAY_SECONDS: "10"
            MAX_ATTEMPTS_PER_ARTICLE: "1"
            PASS_RATE_ENFORCE: "1"
            MIN_PASS_RATE: "0.95"
            PASS_RATE_WINDOW: "20"
            PASS_RATE_MIN_SAMPLES: "10"
            ANTI_DRIFT_REQUEUE_DONE_ON_HARD_FAIL: "1"
            ANTI_DRIFT_RETRY_TERMINAL_ON_HARD_FAIL: "0"
        steps:
            - name: Checkout
              uses: actions/checkout@v4

            - name: Setup Python
              uses: actions/setup-python@v5
              with:
                  python-version: "3.11"

            - name: Install dependencies
              run: |
                  python -m pip install --upgrade pip
                  python -m pip install requests beautifulsoup4 python-dotenv PyYAML google-auth pillow

            - name: Write GCP service account
              run: |
                  python - <<'PY'
                  import os
                  from pathlib import Path
                  data = os.environ.get("GCP_SA_JSON", "").strip()
                  if not data:
                      raise SystemExit(0)
                  path = os.environ.get("GOOGLE_APPLICATION_CREDENTIALS", "")
                  if not path:
                      raise SystemExit("GOOGLE_APPLICATION_CREDENTIALS is not set")
                  Path(path).write_text(data, encoding="utf-8")
                  print(f"Wrote service account to {path}")
                  PY

            - name: Verify vision review enabled
              run: |
                  if [ "${VISION_REVIEW:-0}" = "1" ] \
                      && [ -z "${VISION_API_KEY:-}" ] \
                      && [ -z "${GOOGLE_AI_STUDIO_API_KEY:-}" ] \
                      && [ -z "${GEMINI_API_KEY:-}" ] \
                      && [ -z "${FALLBACK_GEMINI_API_KEY:-}" ] \
                      && [ -z "${SECOND_FALLBACK_GEMINI_API_KEY:-}" ]; then
                          echo "Missing VISION_API_KEY or any GEMINI API key; vision review is required."
                          exit 1
                  fi

            - name: Check if queue needs refresh
              id: queue_check
              working-directory: Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2
              run: |
                  python - <<'PY'
                  import json
                  import os
                  from pathlib import Path

                  output_path = os.environ.get('GITHUB_OUTPUT')
                  run_number = int(os.environ.get('GITHUB_RUN_NUMBER', '0'))

                  queue_path = Path('anti_drift_queue.json')
                  if not queue_path.exists():
                      if output_path:
                          with open(output_path, 'a', encoding='utf-8') as f:
                              f.write('needs_refresh=true\n')
                      raise SystemExit(0)

                  try:
                      data = json.loads(queue_path.read_text(encoding='utf-8'))
                  except json.JSONDecodeError:
                      if output_path:
                          with open(output_path, 'a', encoding='utf-8') as f:
                              f.write('needs_refresh=true\n')
                      raise SystemExit(0)

                  items = data.get('items', [])
                  pending_like = [
                      i for i in items
                      if i.get('status') in {'pending', 'failed', 'retrying', 'manual_review'}
                  ]
                  total = len(items)
                  done_count = len([i for i in items if i.get('status') == 'done'])

                  # Periodic re-scan: every 15 runs (~2.5h), force refresh to catch NEW broken articles
                  force_periodic = (run_number % 15 == 0) and run_number > 0
                  if force_periodic:
                      print(f"[PERIODIC RESCAN] Run #{run_number} — forcing queue refresh")

                  # Smart refresh: if >80% of queue is done, most work is finished → rescan for new issues
                  smart_refresh = total > 0 and (done_count / total) > 0.80
                  if smart_refresh and not force_periodic:
                      print(f"[SMART REFRESH] {done_count}/{total} done ({done_count*100//total}%) — triggering rescan")

                  needs_refresh = 'true' if (not pending_like or force_periodic or smart_refresh) else 'false'
                  if output_path:
                      with open(output_path, 'a', encoding='utf-8') as f:
                          f.write(f'needs_refresh={needs_refresh}\n')
                  PY

            - name: Scan issues (published)
              if: steps.queue_check.outputs.needs_refresh == 'true'
              working-directory: Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2
              run: |
                  SCAN_LIMIT=200 python scan_issues_now.py

            - name: Ensure articles_to_fix.json exists
              if: steps.queue_check.outputs.needs_refresh == 'true'
              id: fix_list
              working-directory: Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2
              run: |
                  python - <<'PY'
                  import json
                  import os
                  from pathlib import Path

                  path = Path('articles_to_fix.json')
                  if not path.exists():
                      path.write_text('[]', encoding='utf-8')

                  try:
                      items = json.loads(path.read_text(encoding='utf-8'))
                  except json.JSONDecodeError:
                      items = []
                      path.write_text('[]', encoding='utf-8')

                  has_items = 'true' if items else 'false'
                  output_path = os.environ.get('GITHUB_OUTPUT')
                  if output_path:
                      with open(output_path, 'a', encoding='utf-8') as f:
                          f.write(f'has_items={has_items}\n')
                  print(f'articles_to_fix count: {len(items)}')
                  PY

            - name: Rotate queue to avoid repeat
              if: steps.queue_check.outputs.needs_refresh == 'true' && steps.fix_list.outputs.has_items == 'true'
              working-directory: Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2
              run: |
                  if [ ! -s "articles_to_fix.json" ]; then
                      echo "No items to fix; skipping rotate."
                      exit 0
                  fi
                  python - <<'PY'
                  import json
                  import os
                  from pathlib import Path

                  path = Path('articles_to_fix.json')
                  if not path.exists():
                      print('articles_to_fix.json not found; skip rotate')
                      raise SystemExit(0)

                  items = json.loads(path.read_text(encoding='utf-8'))
                  if not items:
                      print('No items to fix')
                      raise SystemExit(0)

                  run_number = int(os.environ.get('GITHUB_RUN_NUMBER', '0'))
                  offset = run_number % len(items)
                  rotated = items[offset:] + items[:offset]
                  path.write_text(json.dumps(rotated, ensure_ascii=False, indent=2), encoding='utf-8')
                  print(f'Rotated queue by offset {offset} (run {run_number})')
                  PY

            - name: Init anti-drift queue
              if: steps.queue_check.outputs.needs_refresh == 'true' && steps.fix_list.outputs.has_items == 'true'
              working-directory: Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2
              run: |
                  if [ ! -s "articles_to_fix.json" ]; then
                      echo "No items to fix; skipping queue init."
                      exit 0
                  fi
                  python ai_orchestrator.py queue-init

            - name: Batch FAST-SKIP already-OK articles
              working-directory: Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2
              run: |
                  set -uo pipefail
                  if [ ! -f "anti_drift_queue.json" ]; then
                      echo "No queue file; skipping batch FAST-SKIP."
                      exit 0
                  fi
                  echo "=== BATCH FAST-SKIP: Checking all pending articles ==="
                  python - <<'BATCH_SKIP'
                  import json, os, sys, requests, time
                  from pathlib import Path

                  queue_path = Path('anti_drift_queue.json')
                  data = json.loads(queue_path.read_text(encoding='utf-8'))
                  items = data.get('items', [])
                  # Check ALL non-done items: pending, failed, retrying, manual_review
                  # Any article that is published + no critical errors should be marked done
                  pending = [i for i in items if i.get('status') not in ('done', 'in_progress')]

                  if not pending:
                      print("No items to check for FAST-SKIP.")
                      sys.exit(0)

                  print(f"Checking {len(pending)} non-done articles for FAST-SKIP...")

                  SHOP = os.environ.get('SHOPIFY_SHOP') or os.environ.get('SHOPIFY_STORE_DOMAIN') or ''
                  if SHOP and '.myshopify.com' not in SHOP:
                      SHOP = SHOP + '.myshopify.com'
                  BLOG_ID = os.environ.get('SHOPIFY_BLOG_ID') or os.environ.get('BLOG_ID') or ''
                  TOKEN = os.environ.get('SHOPIFY_ACCESS_TOKEN') or ''
                  API_VERSION = os.environ.get('SHOPIFY_API_VERSION') or '2025-01'

                  GENERIC_PHRASES = [
                      'this comprehensive guide provides', 'this comprehensive guide covers',
                      'whether you are a beginner', "whether you're a beginner",
                      'this practical guide', 'join thousands who have already mastered',
                  ]

                  skipped = 0
                  still_broken = 0
                  not_published = 0
                  errors = 0

                  # Load blacklist for syncing
                  bl_path = Path('anti_drift_done_blacklist.json')
                  done_ids = set()
                  if bl_path.exists():
                      try:
                          bl = json.loads(bl_path.read_text(encoding='utf-8'))
                          done_ids = set(str(x) for x in bl.get('done_ids', []))
                      except Exception:
                          pass

                  for idx, item in enumerate(pending):
                      aid = str(item.get('id'))
                      title = (item.get('title') or '')[:40]
                      if (idx + 1) % 20 == 0:
                          print(f"  ... checked {idx + 1}/{len(pending)}")

                      # Do NOT FAST-SKIP items that were queued as HARD FAIL.
                      # Those must go through queue-step (full fix + review) instead.
                      if (item.get('last_error') or '').strip() in ('HARD_FAIL_SCAN', 'REQUEUED_HARD_FAIL_SCAN'):
                          still_broken += 1
                          continue

                      # Step 1: Check Shopify published status FIRST (fast API call)
                      try:
                          url = f'https://{SHOP}/admin/api/{API_VERSION}/blogs/{BLOG_ID}/articles/{aid}.json'
                          r = requests.get(url, headers={'X-Shopify-Access-Token': TOKEN, 'Content-Type': 'application/json'}, timeout=15)
                          if r.status_code != 200:
                              not_published += 1
                              continue
                          article = r.json().get('article', {})
                          pub = article.get('published_at', '')
                          if not pub:
                              not_published += 1
                              continue
                      except Exception:
                          errors += 1
                          continue

                      # Step 2: Lightweight critical-error check (NO full pre_publish_review)
                      # Only check errors that truly harm SEO/UX on published articles
                      body = article.get('body_html', '') or ''
                      body_lower = body.lower()
                      has_critical_error = False

                      # Check broken images (HTTP 404 img tags)
                      import re as _re
                      img_srcs = _re.findall(r'<img[^>]+src=["\']([^"\'>]+)["\']', body)
                      broken_imgs = 0
                      for src in img_srcs[:5]:  # Check max 5 images to stay fast
                          try:
                              ir = requests.head(src, timeout=8, allow_redirects=True)
                              if ir.status_code >= 400:
                                  broken_imgs += 1
                          except Exception:
                              broken_imgs += 1
                      if broken_imgs > 0:
                          has_critical_error = True
                          print(f"  [{aid}] CRITICAL: {broken_imgs} broken images")

                      # Check featured image (NO_FEATURED = structural fail)
                      if not article.get('image'):
                          has_critical_error = True
                          print(f"  [{aid}] CRITICAL: no featured image")

                      # Check word count (< 1800 matches scan_issues_now HARD_FAIL)
                      from bs4 import BeautifulSoup as _BS
                      soup = _BS(body, 'html.parser')
                      text = soup.get_text(' ', strip=True)
                      wc = len(_re.findall(r'\w+', text))
                      if wc < 1800:
                          has_critical_error = True
                          print(f"  [{aid}] CRITICAL: word count {wc} < 1800")

                      # Check broken text markers (corrupted content)
                      if 'Cdn Shopify' in body or 'Image Pollinations' in body or 'rate limit' in body_lower:
                          has_critical_error = True
                          print(f"  [{aid}] CRITICAL: broken text markers found")

                      # Check generic content (≥3 phrases = spam)
                      generic_count = sum(1 for p in GENERIC_PHRASES if p in body_lower)
                      if generic_count >= 3:
                          has_critical_error = True
                          print(f"  [{aid}] CRITICAL: {generic_count} generic phrases")

                      if has_critical_error:
                          still_broken += 1
                          continue

                      # Published + no critical errors → mark done
                      item['status'] = 'done'
                      item['last_error'] = 'ALREADY_OK_BATCH'
                      item['updated_at'] = __import__('datetime').datetime.now().isoformat()
                      done_ids.add(aid)
                      skipped += 1
                      print(f"  [{aid}] SKIP: published, {wc} words, no critical errors")

                      # Rate limit: small delay every 10 articles to avoid Shopify throttle
                      if (skipped + still_broken) % 10 == 0:
                          time.sleep(1)

                  # Save queue
                  data['items'] = items
                  queue_path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding='utf-8')

                  # Save blacklist
                  from datetime import datetime as dt
                  bl_path.write_text(json.dumps({
                      'updated_at': dt.now().isoformat(),
                      'done_ids': sorted(done_ids)
                  }, ensure_ascii=False, indent=2), encoding='utf-8')

                  # Auto-escalate stuck articles (too many attempts → manual_review)
                  MAX_ESCALATE_ATTEMPTS = 3
                  escalated = 0
                  for item in items:
                      if item.get('status') in ('failed', 'retrying', 'pending') and int(item.get('attempts', 0)) >= MAX_ESCALATE_ATTEMPTS:
                          item['status'] = 'manual_review'
                          item['last_error'] = f"AUTO_ESCALATED_AFTER_{item.get('attempts')}_ATTEMPTS: {item.get('last_error', '')}"
                          item['updated_at'] = __import__('datetime').datetime.now().isoformat()
                          escalated += 1

                  remaining = len([i for i in items if i.get('status') in ('pending', 'failed', 'retrying')])
                  print(f"\n=== BATCH FAST-SKIP RESULTS ===")
                  print(f"  Skipped (already OK): {skipped}")
                  print(f"  Still broken (critical errors): {still_broken}")
                  print(f"  Not published: {not_published}")
                  print(f"  Errors: {errors}")
                  print(f"  Auto-escalated (stuck ≥{MAX_ESCALATE_ATTEMPTS} attempts): {escalated}")
                  print(f"  Remaining to fix: {remaining}")

                  # Save escalated changes
                  data['items'] = items
                  queue_path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding='utf-8')
                  BATCH_SKIP

            - name: Fix articles via queue-step (end-to-end)
              working-directory: Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2
              run: |
                  set -uo pipefail
                  if [ ! -f "anti_drift_queue.json" ]; then
                      echo "Queue missing; re-initializing."
                      python ai_orchestrator.py queue-init || true
                  fi
                  if [ ! -f "anti_drift_queue.json" ]; then
                      echo "No queue found; skipping fix loop."
                      exit 0
                  fi
                  max=${FIX_MAX_ITEMS:-2}
                  delay=${FIX_DELAY_SECONDS:-10}
                  start_time=$(date +%s)
                  time_budget=1800  # 30 min budget (leaves ~15 min for commit + batch skip)

                  # queue-step handles EVERYTHING end-to-end per article:
                  #   structural repair → image fix → strip broken images →
                  #   pre_publish_review → auto-fix → cleanup → publish → queue status
                  for i in $(seq 1 "$max"); do
                      elapsed=$(( $(date +%s) - start_time ))
                      if [ "$elapsed" -ge "$time_budget" ]; then
                          echo "[TIME] Budget exhausted (${elapsed}s >= ${time_budget}s). Stopping."
                          break
                      fi
                      remaining_time=$(( time_budget - elapsed ))
                      echo "[TIME] ${elapsed}s elapsed, ${remaining_time}s remaining"
                      echo "=== queue-step $i/$max ==="
                      set +e
                      timeout 1200 python ai_orchestrator.py queue-step
                      step_exit=$?
                      set -e
                      if [ "$step_exit" = "124" ]; then
                          echo "[WARN] queue-step TIMED OUT after 20 min"
                      elif [ "$step_exit" != "0" ]; then
                          echo "[WARN] queue-step exited $step_exit"
                      fi
                      if [ "$i" -lt "$max" ]; then
                          echo "Sleeping ${delay}s before next article..."
                          sleep "$delay"
                      fi
                  done
                  exit 0

            - name: Commit queue changes
              if: always()
              env:
                  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
              run: |
                  git config user.name "github-actions[bot]"
                  git config user.email "github-actions[bot]@users.noreply.github.com"
                  QUEUE_DIR="Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2"
                  if [ -f "$QUEUE_DIR/anti_drift_queue.json" ]; then
                      # Save our queue state before any git operations
                      cp "$QUEUE_DIR/anti_drift_queue.json" /tmp/queue_ours.json
                      cp "$QUEUE_DIR/anti_drift_done_blacklist.json" /tmp/blacklist_ours.json 2>/dev/null || true

                      git remote set-url origin "https://x-access-token:${GITHUB_TOKEN}@github.com/${{ github.repository }}.git"
                      git stash --include-untracked || true
                      git pull --rebase origin ${{ github.ref_name }} || {
                          echo "[WARN] git pull --rebase failed; resetting to remote"
                          git rebase --abort 2>/dev/null || true
                          git fetch origin ${{ github.ref_name }}
                          git reset --hard origin/${{ github.ref_name }}
                      }
                      git stash pop || {
                          echo "[WARN] git stash pop had conflicts; using our saved queue"
                          git checkout --theirs -- "$QUEUE_DIR/anti_drift_queue.json" 2>/dev/null || true
                          git reset HEAD 2>/dev/null || true
                      }

                      # Auto-resolve: if queue has conflict markers, restore our clean copy
                      if grep -q '<<<<<<<' "$QUEUE_DIR/anti_drift_queue.json" 2>/dev/null; then
                          echo "[WARN] Merge conflict detected in queue — restoring our version"
                          cp /tmp/queue_ours.json "$QUEUE_DIR/anti_drift_queue.json"
                      fi
                      # Validate JSON; if corrupt, restore our clean copy
                      python -c "import json; json.load(open('$QUEUE_DIR/anti_drift_queue.json'))" 2>/dev/null || {
                          echo "[WARN] Queue JSON invalid — restoring our version"
                          cp /tmp/queue_ours.json "$QUEUE_DIR/anti_drift_queue.json"
                      }
                      if [ -f /tmp/blacklist_ours.json ]; then
                          cp /tmp/blacklist_ours.json "$QUEUE_DIR/anti_drift_done_blacklist.json" 2>/dev/null || true
                      fi

                      # Use --force to add files that are in .gitignore
                      git add --force "$QUEUE_DIR/anti_drift_queue.json" || echo "add queue failed"
                      git add --force "$QUEUE_DIR/anti_drift_run_log.csv" 2>/dev/null || true
                      git add --force "$QUEUE_DIR/anti_drift_done_blacklist.json" 2>/dev/null || true
                      git add --force "$QUEUE_DIR/orchestrator_progress.json" 2>/dev/null || true
                      echo "Files staged:"
                      git diff --cached --name-only || true
                      if ! git diff --cached --quiet; then
                          git commit -m "chore: Update queue state after run ${{ github.run_number }} [skip ci]"
                          git push origin HEAD:${{ github.ref_name }} && echo "✅ Queue changes pushed successfully" || echo "❌ Push failed (will retry next run)"
                      else
                          echo "No queue changes to commit (git diff --cached is empty)"
                      fi
                  else
                      echo "Queue file not found at $QUEUE_DIR/anti_drift_queue.json"
                  fi

            - name: Upload fix artifacts
              uses: actions/upload-artifact@v4
              with:
                  name: auto-fix-sequential
                  path: |
                      Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2/anti_drift_queue.json
                      Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2/anti_drift_run_log.csv
                      Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2/orchestrator_progress.json
                      Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2/anti_drift_done_blacklist.json
                      Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2/review-output*.txt
                  if-no-files-found: ignore

            - name: Job summary
              if: always()
              working-directory: Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2
              run: |
                  echo "## Auto Fix Sequential - Run Summary" >> $GITHUB_STEP_SUMMARY
                  echo "" >> $GITHUB_STEP_SUMMARY
                  if [ -f "anti_drift_queue.json" ]; then
                    QUEUE_COUNT=$(python -c "import json; d=json.load(open('anti_drift_queue.json')); print(len(d.get('items',[])))" 2>/dev/null || echo "0")
                    PENDING=$(python -c "import json; d=json.load(open('anti_drift_queue.json')); items=d.get('items',[]); print(len([i for i in items if i.get('status') in {'pending','retrying','failed','manual_review'}]))" 2>/dev/null || echo "0")
                    DONE=$(python -c "import json; d=json.load(open('anti_drift_queue.json')); items=d.get('items',[]); print(len([i for i in items if i.get('status')=='done']))" 2>/dev/null || echo "0")
                    echo "- Queue: $QUEUE_COUNT items | Pending: $PENDING | Done: $DONE" >> $GITHUB_STEP_SUMMARY
                  else
                    echo "- Queue file not found (scan + queue-init run first; check logs above)." >> $GITHUB_STEP_SUMMARY
                  fi
                  echo "" >> $GITHUB_STEP_SUMMARY
                  echo "**Schedule:** This workflow runs every 10 min on the **default branch** only. Set repo default branch to \`feat/l6-reconcile-main\` if you use that branch." >> $GITHUB_STEP_SUMMARY
