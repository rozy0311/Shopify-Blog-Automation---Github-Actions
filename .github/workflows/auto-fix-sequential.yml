name: Auto Fix Sequential (Continuous Loop)
# Runs every 10 minutes
# Batch FAST-SKIP: marks ALL already-published + passing articles as done in one pass
# Then processes up to 2 truly broken articles per run (within 45-min budget)
# cancel-in-progress: true = prevent stacking of scheduled runs

on:
    schedule:
        # Every 10 minutes — 1 article/run, natural 10-min gap between articles
        - cron: "*/10 * * * *"
    workflow_dispatch:
        inputs:
            fix_max_items:
                description: "Number of articles per run (default 2)"
                required: false
                default: "2"

concurrency:
    group: auto-fix-sequential
    cancel-in-progress: true

jobs:
    auto-fix-one:
        runs-on: ubuntu-latest
        timeout-minutes: 45
        permissions:
            contents: write
        env:
            SHOPIFY_SHOP: ${{ secrets.SHOPIFY_SHOP }}
            SHOPIFY_STORE_DOMAIN: ${{ secrets.SHOPIFY_SHOP }}
            SHOPIFY_ACCESS_TOKEN: ${{ secrets.SHOPIFY_ACCESS_TOKEN }}
            SHOPIFY_BLOG_ID: ${{ secrets.SHOPIFY_BLOG_ID }}
            SHOPIFY_API_VERSION: ${{ secrets.SHOPIFY_API_VERSION }}
            POLLINATIONS_API_KEY: ${{ secrets.POLLINATIONS_API_KEY }}
            GET_POLLINATIONS_URL: ${{ secrets.GET_POLLINATIONS_URL }}
            POLLINATIONS_NEGATIVE: "deformed hands, extra fingers, mutated hands, poorly drawn hands, bad anatomy, extra limbs, fused fingers, too many fingers, missing fingers, deformed, blurry, bad proportions, text, watermark, logo, signature"
            USE_LEXICA: "0"
            LEXICA_ONLY_SD15: "1"
            LEXICA_RESULT_LIMIT: "30"
            LEXICA_FALLBACK_ONLY: "1"
            VISION_REVIEW: "1"
            VISION_API_KEY: ${{ secrets.VISION_API_KEY }}
            GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
            GOOGLE_AI_STUDIO_API_KEY: ${{ secrets.GOOGLE_AI_STUDIO_API_KEY }}
            FALLBACK_GEMINI_API_KEY: ${{ secrets.FALLBACK_GEMINI_API_KEY }}
            FALLBACK_GOOGLE_AI_STUDIO_API_KEY: ${{ secrets.FALLBACK_GEMINI_API_KEY }}
            GEMINI_MODEL: gemini-2.0-flash
            GEMINI_MODEL_FALLBACK: gemini-2.5-flash-lite
            GEMINI_MODEL_FALLBACK_2: gemini-2.5-flash
            GEMINI_MODEL_FALLBACK_3: gemini-2.0-flash-lite
            GEMINI_DELAY_SECONDS: "3"
            OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
            GH_MODELS_API_KEY: ${{ secrets.GH_MODELS_API_KEY }}
            GH_MODELS_MODEL: ${{ vars.GH_MODELS_MODEL || 'openai/gpt-4.1' }}
            VISION_PROVIDER: ${{ vars.VISION_PROVIDER || '' }}
            VISION_API_BASE: ${{ vars.VISION_API_BASE || 'https://models.github.ai/inference' }}
            VISION_MODEL_ID: ${{ vars.VISION_MODEL_ID || 'openai/gpt-4o-mini' }}
            VISION_TIMEOUT: ${{ vars.VISION_TIMEOUT || '30' }}
            VISION_MAX_ATTEMPTS: ${{ vars.VISION_MAX_ATTEMPTS || '6' }}
            VISION_RETRY_SLEEP: ${{ vars.VISION_RETRY_SLEEP || '6' }}
            VISION_BACKOFF_FACTOR: ${{ vars.VISION_BACKOFF_FACTOR || '1.8' }}
            GCP_PROJECT: ${{ vars.GCP_PROJECT || '' }}
            GCP_LOCATION: ${{ vars.GCP_LOCATION || 'us-central1' }}
            GEMINI_IMAGE_MODEL: ${{ vars.GEMINI_IMAGE_MODEL || 'imagen-4.0-fast-generate-001' }}
            GOOGLE_APPLICATION_CREDENTIALS: ${{ github.workspace }}/gcp-sa.json
            GCP_SA_JSON: ${{ secrets.GCP_SA_JSON }}
            USE_VERTEX_IMAGEN: ${{ vars.USE_VERTEX_IMAGEN || '' }}
            REQUIRE_PINTEREST_IMAGE: "0"
            FIX_MAX_ITEMS: ${{ github.event.inputs.fix_max_items || '2' }}
            FIX_DELAY_SECONDS: "10"
            MAX_ATTEMPTS_PER_ARTICLE: "1"
            PASS_RATE_ENFORCE: "1"
            MIN_PASS_RATE: "0.95"
            PASS_RATE_WINDOW: "20"
            PASS_RATE_MIN_SAMPLES: "10"
        steps:
            - name: Checkout
              uses: actions/checkout@v4

            - name: Setup Python
              uses: actions/setup-python@v5
              with:
                  python-version: "3.11"

            - name: Install dependencies
              run: |
                  python -m pip install --upgrade pip
                  python -m pip install requests beautifulsoup4 python-dotenv PyYAML google-auth pillow

            - name: Write GCP service account
              run: |
                  python - <<'PY'
                  import os
                  from pathlib import Path
                  data = os.environ.get("GCP_SA_JSON", "").strip()
                  if not data:
                      raise SystemExit(0)
                  path = os.environ.get("GOOGLE_APPLICATION_CREDENTIALS", "")
                  if not path:
                      raise SystemExit("GOOGLE_APPLICATION_CREDENTIALS is not set")
                  Path(path).write_text(data, encoding="utf-8")
                  print(f"Wrote service account to {path}")
                  PY

            - name: Verify vision review enabled
              run: |
                  if [ "${VISION_REVIEW:-0}" = "1" ] && [ -z "${VISION_API_KEY:-}" ] && [ -z "${GEMINI_API_KEY:-}" ]; then
                      echo "Missing VISION_API_KEY or GEMINI_API_KEY; vision review is required."
                      exit 1
                  fi

            - name: Check if queue needs refresh
              id: queue_check
              working-directory: Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2
              run: |
                  python - <<'PY'
                  import json
                  import os
                  from pathlib import Path

                  output_path = os.environ.get('GITHUB_OUTPUT')
                  run_number = int(os.environ.get('GITHUB_RUN_NUMBER', '0'))

                  queue_path = Path('anti_drift_queue.json')
                  if not queue_path.exists():
                      if output_path:
                          with open(output_path, 'a', encoding='utf-8') as f:
                              f.write('needs_refresh=true\n')
                      raise SystemExit(0)

                  try:
                      data = json.loads(queue_path.read_text(encoding='utf-8'))
                  except json.JSONDecodeError:
                      if output_path:
                          with open(output_path, 'a', encoding='utf-8') as f:
                              f.write('needs_refresh=true\n')
                      raise SystemExit(0)

                  items = data.get('items', [])
                  pending_like = [
                      i for i in items
                      if i.get('status') in {'pending', 'failed', 'retrying', 'manual_review'}
                  ]

                  # Periodic re-scan: every 50 runs, force refresh to catch NEW broken articles
                  # (title repeats, low words, regressions from other changes)
                  force_periodic = (run_number % 50 == 0) and run_number > 0
                  if force_periodic:
                      print(f"[PERIODIC RESCAN] Run #{run_number} — forcing queue refresh")

                  needs_refresh = 'true' if (not pending_like or force_periodic) else 'false'
                  if output_path:
                      with open(output_path, 'a', encoding='utf-8') as f:
                          f.write(f'needs_refresh={needs_refresh}\n')
                  PY

            - name: Scan issues (published)
              if: steps.queue_check.outputs.needs_refresh == 'true'
              working-directory: Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2
              run: |
                  SCAN_LIMIT=200 python scan_issues_now.py

            - name: Ensure articles_to_fix.json exists
              if: steps.queue_check.outputs.needs_refresh == 'true'
              id: fix_list
              working-directory: Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2
              run: |
                  python - <<'PY'
                  import json
                  import os
                  from pathlib import Path

                  path = Path('articles_to_fix.json')
                  if not path.exists():
                      path.write_text('[]', encoding='utf-8')

                  try:
                      items = json.loads(path.read_text(encoding='utf-8'))
                  except json.JSONDecodeError:
                      items = []
                      path.write_text('[]', encoding='utf-8')

                  has_items = 'true' if items else 'false'
                  output_path = os.environ.get('GITHUB_OUTPUT')
                  if output_path:
                      with open(output_path, 'a', encoding='utf-8') as f:
                          f.write(f'has_items={has_items}\n')
                  print(f'articles_to_fix count: {len(items)}')
                  PY

            - name: Rotate queue to avoid repeat
              if: steps.queue_check.outputs.needs_refresh == 'true' && steps.fix_list.outputs.has_items == 'true'
              working-directory: Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2
              run: |
                  if [ ! -s "articles_to_fix.json" ]; then
                      echo "No items to fix; skipping rotate."
                      exit 0
                  fi
                  python - <<'PY'
                  import json
                  import os
                  from pathlib import Path

                  path = Path('articles_to_fix.json')
                  if not path.exists():
                      print('articles_to_fix.json not found; skip rotate')
                      raise SystemExit(0)

                  items = json.loads(path.read_text(encoding='utf-8'))
                  if not items:
                      print('No items to fix')
                      raise SystemExit(0)

                  run_number = int(os.environ.get('GITHUB_RUN_NUMBER', '0'))
                  offset = run_number % len(items)
                  rotated = items[offset:] + items[:offset]
                  path.write_text(json.dumps(rotated, ensure_ascii=False, indent=2), encoding='utf-8')
                  print(f'Rotated queue by offset {offset} (run {run_number})')
                  PY

            - name: Init anti-drift queue
              if: steps.queue_check.outputs.needs_refresh == 'true' && steps.fix_list.outputs.has_items == 'true'
              working-directory: Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2
              run: |
                  if [ ! -s "articles_to_fix.json" ]; then
                      echo "No items to fix; skipping queue init."
                      exit 0
                  fi
                  python ai_orchestrator.py queue-init

            - name: Batch FAST-SKIP already-OK articles
              working-directory: Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2
              run: |
                  set -uo pipefail
                  if [ ! -f "anti_drift_queue.json" ]; then
                      echo "No queue file; skipping batch FAST-SKIP."
                      exit 0
                  fi
                  echo "=== BATCH FAST-SKIP: Checking all pending articles ==="
                  python - <<'BATCH_SKIP'
                  import json, os, sys, subprocess, requests, time
                  from pathlib import Path

                  queue_path = Path('anti_drift_queue.json')
                  data = json.loads(queue_path.read_text(encoding='utf-8'))
                  items = data.get('items', [])
                  pending = [i for i in items if i.get('status') in ('pending',)]

                  if not pending:
                      print("No pending items to check.")
                      sys.exit(0)

                  print(f"Checking {len(pending)} pending articles for FAST-SKIP...")

                  SHOP = os.environ.get('SHOPIFY_SHOP') or os.environ.get('SHOPIFY_STORE_DOMAIN') or ''
                  if SHOP and '.myshopify.com' not in SHOP:
                      SHOP = SHOP + '.myshopify.com'
                  BLOG_ID = os.environ.get('SHOPIFY_BLOG_ID') or os.environ.get('BLOG_ID') or ''
                  TOKEN = os.environ.get('SHOPIFY_ACCESS_TOKEN') or ''
                  API_VERSION = os.environ.get('SHOPIFY_API_VERSION') or '2025-01'

                  skipped = 0
                  failed_review = 0
                  not_published = 0
                  errors = 0

                  # Load blacklist for syncing
                  bl_path = Path('anti_drift_done_blacklist.json')
                  done_ids = set()
                  if bl_path.exists():
                      try:
                          bl = json.loads(bl_path.read_text(encoding='utf-8'))
                          done_ids = set(str(x) for x in bl.get('done_ids', []))
                      except Exception:
                          pass

                  for idx, item in enumerate(pending):
                      aid = str(item.get('id'))
                      title = (item.get('title') or '')[:40]
                      if (idx + 1) % 20 == 0:
                          print(f"  ... checked {idx + 1}/{len(pending)}")

                      # Step 1: Run pre_publish_review
                      try:
                          result = subprocess.run(
                              ['python', '../scripts/pre_publish_review.py', aid],
                              capture_output=True, text=True, timeout=120
                          )
                          if result.returncode != 0:
                              failed_review += 1
                              continue
                      except Exception as e:
                          errors += 1
                          continue

                      # Step 2: Check if published on Shopify
                      try:
                          url = f'https://{SHOP}/admin/api/{API_VERSION}/blogs/{BLOG_ID}/articles/{aid}.json'
                          r = requests.get(url, headers={'X-Shopify-Access-Token': TOKEN, 'Content-Type': 'application/json'}, timeout=15)
                          if r.status_code != 200:
                              not_published += 1
                              continue
                          pub = r.json().get('article', {}).get('published_at', '')
                          if not pub:
                              not_published += 1
                              continue
                      except Exception:
                          errors += 1
                          continue

                      # Both checks pass → mark done
                      item['status'] = 'done'
                      item['last_error'] = 'ALREADY_OK_BATCH'
                      item['updated_at'] = __import__('datetime').datetime.now().isoformat()
                      done_ids.add(aid)
                      skipped += 1

                      # Rate limit: small delay every 10 articles to avoid Shopify throttle
                      if skipped % 10 == 0:
                          time.sleep(1)

                  # Save queue
                  data['items'] = items
                  queue_path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding='utf-8')

                  # Save blacklist
                  from datetime import datetime as dt
                  bl_path.write_text(json.dumps({
                      'updated_at': dt.now().isoformat(),
                      'done_ids': sorted(done_ids)
                  }, ensure_ascii=False, indent=2), encoding='utf-8')

                  # Auto-escalate stuck articles (too many attempts → manual_review)
                  MAX_ESCALATE_ATTEMPTS = 5
                  escalated = 0
                  for item in items:
                      if item.get('status') in ('failed', 'retrying', 'pending') and int(item.get('attempts', 0)) >= MAX_ESCALATE_ATTEMPTS:
                          item['status'] = 'manual_review'
                          item['last_error'] = f"AUTO_ESCALATED_AFTER_{item.get('attempts')}_ATTEMPTS: {item.get('last_error', '')}"
                          item['updated_at'] = __import__('datetime').datetime.now().isoformat()
                          escalated += 1

                  remaining = len([i for i in items if i.get('status') in ('pending', 'failed', 'retrying')])
                  print(f"\n=== BATCH FAST-SKIP RESULTS ===")
                  print(f"  Skipped (already OK): {skipped}")
                  print(f"  Failed review (need fix): {failed_review}")
                  print(f"  Not published: {not_published}")
                  print(f"  Errors: {errors}")
                  print(f"  Auto-escalated (stuck ≥{MAX_ESCALATE_ATTEMPTS} attempts): {escalated}")
                  print(f"  Remaining to fix: {remaining}")

                  # Save escalated changes
                  data['items'] = items
                  queue_path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding='utf-8')
                  BATCH_SKIP

            - name: Fix up to 50 articles sequentially (no batch)
              working-directory: Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2
              run: |
                  set -uo pipefail
                  if [ ! -f "anti_drift_queue.json" ]; then
                      echo "Queue missing; re-initializing."
                      python ai_orchestrator.py queue-init || true
                  fi
                  if [ ! -f "anti_drift_queue.json" ]; then
                      echo "No queue found; skipping fix loop."
                      exit 0
                  fi
                  max=${FIX_MAX_ITEMS:-2}
                  delay=${FIX_DELAY_SECONDS:-10}
                  processed_file=".processed_ids"
                  start_time=$(date +%s)
                  time_budget=1800  # 30 min budget for fix loop (leaves ~15 min for commit + batch skip)
                  : > "$processed_file"
                  for i in $(seq 1 "$max"); do
                      # Time budget check — stop early so commit step can run
                      elapsed=$(( $(date +%s) - start_time ))
                      if [ "$elapsed" -ge "$time_budget" ]; then
                          echo "[TIME] Budget exhausted (${elapsed}s >= ${time_budget}s). Stopping to allow commit."
                          break
                      fi
                      remaining_time=$(( time_budget - elapsed ))
                      echo "[TIME] ${elapsed}s elapsed, ${remaining_time}s remaining"
                      article_id=$(python - <<'PY'
                  import json
                  from pathlib import Path

                  queue_path = Path('anti_drift_queue.json')
                  if not queue_path.exists():
                      raise SystemExit('')

                  data = json.loads(queue_path.read_text(encoding='utf-8'))
                  items = data.get('items', [])
                  MAX_PICK_ATTEMPTS = 5  # Stop re-processing after this many attempts
                  priority = ['pending', 'retrying', 'failed']
                  chosen = None
                  for status in priority:
                      for item in items:
                          if item.get('status') == status:
                              attempts = int(item.get('attempts', 0))
                              if attempts >= MAX_PICK_ATTEMPTS:
                                  continue  # Skip stuck articles
                              chosen = item
                              break
                      if chosen:
                          break

                  print(chosen.get('id') if chosen else '')
                  PY
                      )

                      if [ -z "$article_id" ]; then
                          echo "No eligible items in queue."
                          break
                      fi

                      if grep -Fxq "$article_id" "$processed_file" 2>/dev/null; then
                          echo "Already processed $article_id in this run; skipping."
                          python - <<PY
                  import json
                  from pathlib import Path
                  article_id = "$article_id"
                  queue_path = Path('anti_drift_queue.json')
                  data = json.loads(queue_path.read_text(encoding='utf-8'))
                  for item in data.get('items', []):
                      if str(item.get('id')) == str(article_id):
                          item['status'] = 'skipped'
                          item['error'] = 'DUP_IN_RUN'
                  queue_path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding='utf-8')
                  PY
                          continue
                      fi
                      echo "Processing $article_id ($i/$max)"
                      echo "$article_id" >> "$processed_file"

                      set +e
                      timeout 300 python ai_orchestrator.py fix-ids "$article_id"
                      fix_ok=$?
                      if [ "$fix_ok" = "124" ]; then echo "[WARN] fix-ids $article_id TIMED OUT after 5 min"; fi
                      if [ "$fix_ok" != "0" ]; then echo "[WARN] fix-ids $article_id exited $fix_ok"; fi
                      # NOTE: Removed build_meta_fix_queue.py and run_meta_fix_queue.py
                      # as ai_orchestrator.py now handles all meta-prompt fixes including:
                      # - Generic phrase removal
                      # - Source injection
                      # - Word count enforcement
                      # - Featured image counting
                      timeout 300 python "fix_images_properly.py" --article-id "$article_id" --images-only
                      fix_images_ok=$?
                      if [ "$fix_images_ok" = "124" ]; then
                          echo "[WARN] fix_images_properly TIMED OUT after 5 min for $article_id"
                          python ai_orchestrator.py queue-review "$article_id" fail "IMAGE_FIX_TIMEOUT" || true
                          if [ "$i" -lt "$max" ]; then
                              echo "Sleeping ${delay}s before next article..."
                              sleep "$delay"
                          fi
                          continue
                      fi

                      # Post-image cleanup: remove title spam reintroduced by fix_images alt/figcaption
                      timeout 60 python post_image_cleanup.py "$article_id" || echo "[WARN] Post-image cleanup failed (non-critical)"
                      set -e

                      set +e
                      timeout 180 python "../scripts/pre_publish_review.py" "$article_id" | tee "review-output-$article_id.txt"
                      review_status=${PIPESTATUS[0]}
                      set -e

                      if [ "$review_status" != "0" ]; then
                          # Retry loop with MAX_ATTEMPTS_PER_ARTICLE
                          max_attempts=${MAX_ATTEMPTS_PER_ARTICLE:-3}
                          attempt=1
                          review_passed=0
                          while [ "$attempt" -le "$max_attempts" ] && [ "$review_passed" -eq 0 ]; do
                              echo "[Attempt $attempt/$max_attempts] Rebuilding $article_id..."
                              set +e
                              timeout 300 python ai_orchestrator.py force-rebuild-ids "$article_id"
                              timeout 300 python "fix_images_properly.py" --article-id "$article_id" --images-only
                              rebuild_images_ok=$?
                              set -e
                              if [ "$rebuild_images_ok" != "0" ]; then
                                  echo "[WARN] fix_images_properly failed on attempt $attempt for $article_id"
                                  attempt=$((attempt + 1))
                                  sleep 30
                                  continue
                              fi
                              # Post-image cleanup (retry): remove title spam reintroduced by images
                              set +e
                              timeout 60 python post_image_cleanup.py "$article_id" || echo "[WARN] Post-image cleanup failed (non-critical)"
                              set -e
                              set +e
                              timeout 180 python "../scripts/pre_publish_review.py" "$article_id" | tee "review-output-${article_id}-attempt-${attempt}.txt"
                              review_status_attempt=${PIPESTATUS[0]}
                              set -e
                              if [ "$review_status_attempt" == "0" ]; then
                                  review_passed=1
                                  echo "[SUCCESS] Review passed on attempt $attempt for $article_id"
                              else
                                  echo "[WARN] Review failed on attempt $attempt for $article_id"
                                  attempt=$((attempt + 1))
                                  sleep 30
                              fi
                          done
                          if [ "$review_passed" -eq 0 ]; then
                              python ai_orchestrator.py queue-review "$article_id" fail "PRE_PUBLISH_REVIEW_FAIL_AFTER_${max_attempts}_ATTEMPTS" || true
                              echo "Pre-publish review failed after $max_attempts attempts for $article_id. Skipping."
                          fi
                          review_status_after=$review_passed
                      fi
                      if [ "$review_status" == "0" ] || [ "${review_status_after:-0}" == "1" ]; then
                          echo "Review passed for $article_id. Cleanup then publish..."
                          python cleanup_before_publish.py "$article_id" || true
                          python publish_now_graphql.py "$article_id" || true

                          # POST-PUBLISH VALIDATION: Re-check the article after publish
                          # Catches any issues introduced by cleanup_before_publish or missed by review
                          set +e
                          timeout 120 python "../scripts/pre_publish_review.py" "$article_id" > /dev/null 2>&1
                          post_pub_status=$?
                          set -e
                          if [ "$post_pub_status" != "0" ]; then
                              echo "⚠️ POST-PUBLISH WARNING: Article $article_id failed validation after publish. Will be re-queued on next rescan."
                              python ai_orchestrator.py queue-review "$article_id" done "PUBLISHED_WITH_WARNINGS" || true
                          else
                              python ai_orchestrator.py queue-review "$article_id" done "PUBLISHED" || true
                          fi
                          echo "Published and marked done: $article_id"
                      fi
                      if [ "$i" -lt "$max" ]; then
                          echo "Sleeping ${delay}s before next article..."
                          sleep "$delay"
                      fi
                  done
                  exit 0

            - name: Commit queue changes
              if: always()
              env:
                  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
              run: |
                  git config user.name "github-actions[bot]"
                  git config user.email "github-actions[bot]@users.noreply.github.com"
                  QUEUE_DIR="Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2"
                  if [ -f "$QUEUE_DIR/anti_drift_queue.json" ]; then
                      # Save our queue state before any git operations
                      cp "$QUEUE_DIR/anti_drift_queue.json" /tmp/queue_ours.json
                      cp "$QUEUE_DIR/anti_drift_done_blacklist.json" /tmp/blacklist_ours.json 2>/dev/null || true

                      git remote set-url origin "https://x-access-token:${GITHUB_TOKEN}@github.com/${{ github.repository }}.git"
                      git stash --include-untracked || true
                      git pull --rebase origin ${{ github.ref_name }} || {
                          echo "[WARN] git pull --rebase failed; resetting to remote"
                          git rebase --abort 2>/dev/null || true
                          git fetch origin ${{ github.ref_name }}
                          git reset --hard origin/${{ github.ref_name }}
                      }
                      git stash pop || {
                          echo "[WARN] git stash pop had conflicts; using our saved queue"
                          git checkout --theirs -- "$QUEUE_DIR/anti_drift_queue.json" 2>/dev/null || true
                          git reset HEAD 2>/dev/null || true
                      }

                      # Auto-resolve: if queue has conflict markers, restore our clean copy
                      if grep -q '<<<<<<<' "$QUEUE_DIR/anti_drift_queue.json" 2>/dev/null; then
                          echo "[WARN] Merge conflict detected in queue — restoring our version"
                          cp /tmp/queue_ours.json "$QUEUE_DIR/anti_drift_queue.json"
                      fi
                      # Validate JSON; if corrupt, restore our clean copy
                      python -c "import json; json.load(open('$QUEUE_DIR/anti_drift_queue.json'))" 2>/dev/null || {
                          echo "[WARN] Queue JSON invalid — restoring our version"
                          cp /tmp/queue_ours.json "$QUEUE_DIR/anti_drift_queue.json"
                      }
                      if [ -f /tmp/blacklist_ours.json ]; then
                          cp /tmp/blacklist_ours.json "$QUEUE_DIR/anti_drift_done_blacklist.json" 2>/dev/null || true
                      fi

                      # Use --force to add files that are in .gitignore
                      git add --force "$QUEUE_DIR/anti_drift_queue.json" || echo "add queue failed"
                      git add --force "$QUEUE_DIR/anti_drift_run_log.csv" 2>/dev/null || true
                      git add --force "$QUEUE_DIR/anti_drift_done_blacklist.json" 2>/dev/null || true
                      git add --force "$QUEUE_DIR/orchestrator_progress.json" 2>/dev/null || true
                      echo "Files staged:"
                      git diff --cached --name-only || true
                      if ! git diff --cached --quiet; then
                          git commit -m "chore: Update queue state after run ${{ github.run_number }} [skip ci]"
                          git push origin HEAD:${{ github.ref_name }} && echo "✅ Queue changes pushed successfully" || echo "❌ Push failed (will retry next run)"
                      else
                          echo "No queue changes to commit (git diff --cached is empty)"
                      fi
                  else
                      echo "Queue file not found at $QUEUE_DIR/anti_drift_queue.json"
                  fi

            - name: Upload fix artifacts
              uses: actions/upload-artifact@v4
              with:
                  name: auto-fix-sequential
                  path: |
                      Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2/anti_drift_queue.json
                      Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2/anti_drift_run_log.csv
                      Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2/orchestrator_progress.json
                      Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2/anti_drift_done_blacklist.json
                      Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2/review-output*.txt
                  if-no-files-found: ignore

            - name: Job summary
              if: always()
              working-directory: Agent - Pinterest -Shopify Blog Autopilot - End-to-End Content Factory/pipeline_v2
              run: |
                  echo "## Auto Fix Sequential - Run Summary" >> $GITHUB_STEP_SUMMARY
                  echo "" >> $GITHUB_STEP_SUMMARY
                  if [ -f "anti_drift_queue.json" ]; then
                    QUEUE_COUNT=$(python -c "import json; d=json.load(open('anti_drift_queue.json')); print(len(d.get('items',[])))" 2>/dev/null || echo "0")
                    PENDING=$(python -c "import json; d=json.load(open('anti_drift_queue.json')); items=d.get('items',[]); print(len([i for i in items if i.get('status') in {'pending','retrying','failed','manual_review'}]))" 2>/dev/null || echo "0")
                    DONE=$(python -c "import json; d=json.load(open('anti_drift_queue.json')); items=d.get('items',[]); print(len([i for i in items if i.get('status')=='done']))" 2>/dev/null || echo "0")
                    echo "- Queue: $QUEUE_COUNT items | Pending: $PENDING | Done: $DONE" >> $GITHUB_STEP_SUMMARY
                  else
                    echo "- Queue file not found (scan + queue-init run first; check logs above)." >> $GITHUB_STEP_SUMMARY
                  fi
                  echo "" >> $GITHUB_STEP_SUMMARY
                  echo "**Schedule:** This workflow runs every 10 min on the **default branch** only. Set repo default branch to \`feat/l6-reconcile-main\` if you use that branch." >> $GITHUB_STEP_SUMMARY
